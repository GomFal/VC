{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "\n",
    "IMG_PATH = \"image.jpg\"\n",
    "API_KEY = os.environ[\"ROBOFLOW_API_KEY\"]\n",
    "DISTANCE_TO_OBJECT = 1250  # mm             # Mejor funcionamieto - 50cm reales de la camara\n",
    "HEIGHT_OF_HUMAN_FACE = 210  # mm\n",
    "GAZE_DETECTION_URL = (\n",
    "    \"http://127.0.0.1:9001/gaze/gaze_detection?api_key=\" + API_KEY\n",
    ")\n",
    "previous_gaze_point = None  # Para almacenar el punto de mirada anterior\n",
    "SMOOTHING_FACTOR = 0.7  # Factor de suavizado (ajusta este valor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CALIBRATION] Look at point (0, 0) for 3 seconds.\n",
      "[CALIBRATION] Look at point (480, 0) for 3 seconds.\n"
     ]
    }
   ],
   "source": [
    "def detect_gazes(frame: np.ndarray):\n",
    "    \"\"\"\n",
    "    Sends an image frame to the Roboflow API for gaze detection.\n",
    "    \"\"\"\n",
    "    img_encode = cv2.imencode(\".jpg\", frame)[1]\n",
    "    img_base64 = base64.b64encode(img_encode)\n",
    "    resp = requests.post(\n",
    "        GAZE_DETECTION_URL,\n",
    "        json={\n",
    "            \"api_key\": API_KEY,\n",
    "            \"image\": {\"type\": \"base64\", \"value\": img_base64.decode(\"utf-8\")},\n",
    "        },\n",
    "    )\n",
    "    if resp.status_code != 200:\n",
    "        print(\"[ERROR] Failed to get gaze predictions from Roboflow API.\")\n",
    "        return []\n",
    "    return resp.json()[0].get(\"predictions\", [])\n",
    "\n",
    "def calibrate_with_roboflow(cap, calibration_points, capture_time=3):\n",
    "    \"\"\"\n",
    "    Calibrates the gaze detection by mapping gaze predictions (yaw, pitch)\n",
    "    to screen coordinates using user-provided calibration points.\n",
    "    \"\"\"\n",
    "    calibration_data = []\n",
    "\n",
    "    for (sx, sy) in calibration_points:\n",
    "        print(f\"[CALIBRATION] Look at point ({sx}, {sy}) for {capture_time} seconds.\")\n",
    "        start_time = cv2.getTickCount()\n",
    "\n",
    "        while (cv2.getTickCount() - start_time) / cv2.getTickFrequency() < capture_time:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"[ERROR] Failed to capture frame during calibration.\")\n",
    "                break\n",
    "\n",
    "            # Display the calibration point\n",
    "            canvas = np.zeros_like(frame)\n",
    "            cv2.circle(canvas, (sx, sy), 10, (0, 0, 255), -1)\n",
    "            cv2.imshow(\"Calibration\", canvas)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "            # Get gaze data from Roboflow\n",
    "            gazes = detect_gazes(frame)\n",
    "            if gazes:\n",
    "                gaze = gazes[0]\n",
    "                yaw, pitch = gaze[\"yaw\"], gaze[\"pitch\"]\n",
    "                calibration_data.append(((yaw, pitch), (sx, sy)))\n",
    "\n",
    "    # Create interpolation functions\n",
    "    yaw_pitch = [data[0] for data in calibration_data]\n",
    "    screen_points = [data[1] for data in calibration_data]\n",
    "    f_interp_x = griddata(yaw_pitch, [p[0] for p in screen_points], method=\"linear\", fill_value=\"extrapolate\")\n",
    "    f_interp_y = griddata(yaw_pitch, [p[1] for p in screen_points], method=\"linear\", fill_value=\"extrapolate\")\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    return f_interp_x, f_interp_y\n",
    "\n",
    "def smooth_gaze_point(gaze_point, previous_gaze_point, screen_width, screen_height):\n",
    "    \"\"\"\n",
    "    Smoothens the gaze point using exponential smoothing.\n",
    "    \"\"\"\n",
    "    if previous_gaze_point is None:\n",
    "        return gaze_point\n",
    "\n",
    "    smoothed_x = int(SMOOTHING_FACTOR * previous_gaze_point[0] + (1 - SMOOTHING_FACTOR) * gaze_point[0])\n",
    "    smoothed_y = int(SMOOTHING_FACTOR * previous_gaze_point[1] + (1 - SMOOTHING_FACTOR) * gaze_point[1])\n",
    "\n",
    "    smoothed_x = max(0, min(smoothed_x, screen_width - 1))\n",
    "    smoothed_y = max(0, min(smoothed_y, screen_height - 1))\n",
    "    return smoothed_x, smoothed_y\n",
    "\n",
    "def main():\n",
    "    # Camera setup\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"[ERROR] Unable to open the camera.\")\n",
    "        return\n",
    "\n",
    "    # Screen resolution\n",
    "    screen_width, screen_height = 1920, 1080\n",
    "\n",
    "    # Define calibration points\n",
    "    x_positions = [0, screen_width // 4, screen_width // 2, 3 * screen_width // 4, screen_width - 1]\n",
    "    calibration_points = []\n",
    "    for y in [0, screen_height // 2, screen_height - 1]:\n",
    "        for x in x_positions:\n",
    "            calibration_points.append((x, y))\n",
    "\n",
    "    # Calibration\n",
    "    f_interp_x, f_interp_y = calibrate_with_roboflow(cap, calibration_points)\n",
    "\n",
    "    global previous_gaze_point\n",
    "\n",
    "    print(\"[INFO] Starting gaze tracking. Press ESC to exit.\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"[ERROR] Failed to capture frame.\")\n",
    "            break\n",
    "\n",
    "        # Detect gaze\n",
    "        gazes = detect_gazes(frame)\n",
    "        if not gazes:\n",
    "            continue\n",
    "\n",
    "        gaze = gazes[0]\n",
    "        yaw, pitch = gaze[\"yaw\"], gaze[\"pitch\"]\n",
    "        face_bbox = gaze[\"face\"]\n",
    "\n",
    "        # Calculate gaze point in screen coordinates\n",
    "        dx = -DISTANCE_TO_OBJECT * np.tan(yaw) / (HEIGHT_OF_HUMAN_FACE / face_bbox[\"height\"])\n",
    "        dy = -DISTANCE_TO_OBJECT * np.tan(pitch) / (HEIGHT_OF_HUMAN_FACE / face_bbox[\"height\"])\n",
    "        gaze_point = int(screen_width / 2 + dx), int(screen_height / 2 + dy)\n",
    "\n",
    "        # Map to calibrated screen coordinates\n",
    "        gaze_point = (int(f_interp_x([yaw, pitch])), int(f_interp_y([yaw, pitch])))\n",
    "\n",
    "        # Smooth the gaze point\n",
    "        smoothed_gaze_point = smooth_gaze_point(gaze_point, previous_gaze_point, screen_width, screen_height)\n",
    "        previous_gaze_point = smoothed_gaze_point\n",
    "\n",
    "        # Visualize the gaze point\n",
    "        canvas = np.zeros((screen_height, screen_width, 3), dtype=np.uint8)\n",
    "        cv2.circle(canvas, smoothed_gaze_point, 20, (0, 0, 255), -1)\n",
    "        cv2.imshow(\"Gaze Tracking\", canvas)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == 27:  # ESC key\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roboflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
